#!/usr/bin/env python3
"""
Script para ingerir dados sobre Web App Builders no sistema RAG
Utiliza o arquivo inject_rag.json como fonte de dados
"""

import os
import sys
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente do arquivo .env
load_dotenv()

# Imports do sistema RAG
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from config.config import RAGConfig
from indexing.chunker import ContentChunker
from indexing.embeddings import EmbeddingGenerator
from indexing.index_manager import IndexManager
from ingestion.collector import SourceCollector
import aiohttp
import requests
from bs4 import BeautifulSoup

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WebBuilderIngestor:
    """Ingestor especializado para dados de Web App Builders"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Obter API key do ambiente
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY nÃ£o encontrada nas variÃ¡veis de ambiente")
        
        # Inicializar componentes do RAG
        self.chunker = ContentChunker()
        self.embedding_generator = EmbeddingGenerator(api_key)
        self.index_manager = IndexManager(api_key)
        
        # DiretÃ³rio de dados
        self.data_path = Path("rag_data")
        self.data_path.mkdir(exist_ok=True)
        
        # EstatÃ­sticas
        self.stats = {
            "total_sources": 0,
            "processed_sources": 0,
            "failed_sources": 0,
            "total_chunks": 0,
            "start_time": datetime.now().isoformat()
        }
    
    async def fetch_content(self, session: aiohttp.ClientSession, source: Dict[str, Any]) -> str:
        """Buscar conteÃºdo de uma fonte"""
        try:
            url = source["url"]
            fetch_type = source.get("fetch", "http")
            
            if fetch_type == "http":
                async with session.get(url, timeout=30) as response:
                    if response.status == 200:
                        content = await response.text()
                        logger.info(f"âœ… Fetched: {source['title']}")
                        return content
                    else:
                        logger.warning(f"âŒ HTTP {response.status}: {source['title']}")
                        return ""
            
            elif fetch_type == "git":
                # Para repositÃ³rios Git, usar apenas README por enquanto
                readme_url = url.replace("github.com", "raw.githubusercontent.com") + "/main/README.md"
                try:
                    async with session.get(readme_url, timeout=30) as response:
                        if response.status == 200:
                            content = await response.text()
                            logger.info(f"âœ… Fetched README: {source['title']}")
                            return content
                except:
                    pass
                
                # Fallback para pÃ¡gina do GitHub
                async with session.get(url, timeout=30) as response:
                    if response.status == 200:
                        content = await response.text()
                        logger.info(f"âœ… Fetched GitHub page: {source['title']}")
                        return content
            
            return ""
            
        except Exception as e:
            logger.error(f"âŒ Error fetching {source['title']}: {str(e)}")
            return ""
    
    def clean_content(self, content: str, source: Dict[str, Any]) -> str:
        """Limpar e preparar conteÃºdo para ingestÃ£o"""
        if not content:
            return ""
        
        # Remover HTML tags se necessÃ¡rio
        if source.get("format_hint") == "html":
            import re
            # Remover scripts e styles
            content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
            content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
            # Remover tags HTML bÃ¡sicas
            content = re.sub(r'<[^>]+>', ' ', content)
            # Limpar espaÃ§os extras
            content = re.sub(r'\s+', ' ', content)
        
        return content.strip()
    
    def create_metadata(self, source: Dict[str, Any], chunk_index: int) -> Dict[str, Any]:
        """Criar metadata para um chunk"""
        return {
            "source_title": source["title"],
            "source_url": source["url"],
            "source_type": source["type"],
            "tags": source.get("tags", []),
            "priority": source.get("priority", "medium"),
            "license": source.get("license", "unknown"),
            "notes": source.get("notes", ""),
            "chunk_index": chunk_index,
            "ingestion_date": datetime.now().isoformat(),
            "category": "web_app_builders"
        }
    
    async def process_source(self, session: aiohttp.ClientSession, source: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Processar uma fonte individual"""
        try:
            # Buscar conteÃºdo
            raw_content = await self.fetch_content(session, source)
            if not raw_content:
                self.stats["failed_sources"] += 1
                return []
            
            # Limpar conteÃºdo
            clean_content = self.clean_content(raw_content, source)
            if not clean_content or len(clean_content) < 100:
                logger.warning(f"âš ï¸ Content too short: {source['title']}")
                self.stats["failed_sources"] += 1
                return []
            
            # Prepara conteÃºdo estruturado para chunking
            content_data = {
                'title': source['title'],
                'metadata': {
                    'source_title': source['title'],
                    'source_url': source['url'],
                    'source_type': source['type'],
                    'tags': source.get('tags', []),
                    'priority': source.get('priority', 'medium'),
                    'ingestion_date': datetime.now().isoformat()
                },
                'sections': [{
                    'title': source['title'],
                    'content': clean_content,
                    'section_type': 'main',
                    'importance_score': 0.8
                }]
            }
            
            # Chunka o conteÃºdo
            chunks = self.chunker.chunk_content(content_data)
            
            # Converte chunks para formato estruturado
            processed_chunks = []
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "content": chunk.content,
                    "metadata": chunk.metadata,
                    "source_id": source["url"],
                    "chunk_id": chunk.id or f"{source['url']}#{i}"
                }
                processed_chunks.append(chunk_data)
            
            self.stats["processed_sources"] += 1
            self.stats["total_chunks"] += len(processed_chunks)
            
            logger.info(f"âœ… Processed {len(processed_chunks)} chunks from: {source['title']}")
            return processed_chunks
            
        except Exception as e:
            logger.error(f"âŒ Error processing {source['title']}: {str(e)}")
            self.stats["failed_sources"] += 1
            return []
    
    async def ingest_sources(self, sources: List[Dict[str, Any]]) -> None:
        """Ingerir todas as fontes"""
        self.stats["total_sources"] = len(sources)
        
        # Filtrar por prioridade (high primeiro)
        high_priority = [s for s in sources if s.get("priority") == "high"]
        medium_priority = [s for s in sources if s.get("priority") == "medium"]
        low_priority = [s for s in sources if s.get("priority") == "low"]
        
        ordered_sources = high_priority + medium_priority + low_priority
        
        logger.info(f"ğŸš€ Starting ingestion of {len(ordered_sources)} sources...")
        logger.info(f"ğŸ“Š Priority breakdown: High={len(high_priority)}, Medium={len(medium_priority)}, Low={len(low_priority)}")
        
        all_chunks = []
        
        async with aiohttp.ClientSession() as session:
            # Processar em lotes para evitar sobrecarga
            batch_size = 5
            for i in range(0, len(ordered_sources), batch_size):
                batch = ordered_sources[i:i + batch_size]
                logger.info(f"ğŸ“¦ Processing batch {i//batch_size + 1}/{(len(ordered_sources) + batch_size - 1)//batch_size}")
                
                # Processar lote
                tasks = [self.process_source(session, source) for source in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Coletar chunks vÃ¡lidos
                for result in batch_results:
                    if isinstance(result, list):
                        all_chunks.extend(result)
                
                # Pequena pausa entre lotes
                await asyncio.sleep(1)
        
        # Indexar todos os chunks
        if all_chunks:
            logger.info(f"ğŸ“š Indexing {len(all_chunks)} chunks...")
            await self.index_chunks(all_chunks)
        
        # Salvar estatÃ­sticas
        self.stats["end_time"] = datetime.now().isoformat()
        stats_file = self.data_path / "web_builders_ingestion_stats.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Ingestion completed!")
        logger.info(f"ğŸ“Š Stats: {self.stats['processed_sources']}/{self.stats['total_sources']} sources, {self.stats['total_chunks']} chunks")
    
    async def index_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """Indexar chunks no sistema RAG"""
        try:
            # Preparar dados para indexaÃ§Ã£o
            texts = [chunk["content"] for chunk in chunks]
            metadatas = [chunk["metadata"] for chunk in chunks]
            
            # Gerar embeddings
            logger.info("ğŸ”„ Generating embeddings...")
            embeddings = await self.embedding_generator.generate_embeddings(texts)
            
            # Indexar no vector store
            logger.info("ğŸ”„ Indexing vectors...")
            self.vector_indexer.add_vectors(embeddings, metadatas)
            
            # Indexar no text store
            logger.info("ğŸ”„ Indexing text...")
            for chunk in chunks:
                self.text_indexer.add_document(
                    doc_id=chunk["chunk_id"],
                    content=chunk["content"],
                    metadata=chunk["metadata"]
                )
            
            # Salvar Ã­ndices
            logger.info("ğŸ’¾ Saving indices...")
            self.vector_indexer.save_index()
            self.text_indexer.save_index()
            
            logger.info("âœ… Indexing completed successfully!")
            
        except Exception as e:
            logger.error(f"âŒ Error during indexing: {str(e)}")
            raise

async def main():
    """FunÃ§Ã£o principal"""
    # Carregar dados do inject_rag.json
    inject_file = Path("inject_rag.json")
    if not inject_file.exists():
        logger.error(f"âŒ File not found: {inject_file}")
        return
    
    with open(inject_file, "r", encoding="utf-8") as f:
        sources = json.load(f)
    
    logger.info(f"ğŸ“‹ Loaded {len(sources)} sources from {inject_file}")
    
    # Criar ingestor e processar
    ingestor = WebBuilderIngestor()
    await ingestor.ingest_sources(sources)
    
    logger.info("ğŸ‰ Web App Builders data ingestion completed!")

if __name__ == "__main__":
    asyncio.run(main())