import json
import os
import faiss
import asyncio
from datetime import datetime
from src.indexing.index_manager import IndexManager
from src.ingestion.pipeline import IngestionPipeline
from config.config import RAGConfig

def recover_metadata():
    """Tenta recuperar metadados b√°sicos baseado no √≠ndice FAISS"""
    print("=== Recupera√ß√£o de Metadados ===")
    
    index_file = 'rag_data/faiss_index.bin'
    metadata_file = 'rag_data/chunk_metadata.json'
    
    if not os.path.exists(index_file):
        print("‚úó √çndice FAISS n√£o encontrado")
        return False
    
    try:
        # Carregar √≠ndice FAISS
        index = faiss.read_index(index_file)
        total_vectors = index.ntotal
        
        print(f"‚úì √çndice FAISS carregado: {total_vectors:,} vetores")
        
        # Criar metadados b√°sicos para continuar
        # Como o arquivo est√° corrompido, vamos criar um backup e come√ßar novo
        if os.path.exists(metadata_file):
            backup_file = f"{metadata_file}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            os.rename(metadata_file, backup_file)
            print(f"‚úì Backup do arquivo corrompido criado: {backup_file}")
        
        # Criar novo arquivo de metadados vazio
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump({}, f)
        
        print(f"‚úì Novo arquivo de metadados criado")
        print(f"‚ö†Ô∏è  ATEN√á√ÉO: {total_vectors:,} chunks j√° foram processados")
        print(f"‚ö†Ô∏è  O processo continuar√° do in√≠cio, mas pode pular chunks j√° indexados")
        
        return True
        
    except Exception as e:
        print(f"‚úó Erro na recupera√ß√£o: {e}")
        return False

async def continue_ingestion():
    """Continua o processo de ingest√£o"""
    print("\n=== Continuando Ingest√£o ===")
    
    try:
        # Inicializar pipeline
        pipeline = IngestionPipeline()
        
        # Executar pipeline completo
        print("üöÄ Iniciando pipeline de ingest√£o...")
        stats = await pipeline.run_full_pipeline()
        
        print(f"\n‚úÖ Pipeline conclu√≠do!")
        print(f"üìä Estat√≠sticas:")
        print(f"   - Fontes coletadas: {stats.collected_sources}")
        print(f"   - Fontes normalizadas: {stats.normalized_sources}")
        print(f"   - Fontes v√°lidas: {stats.valid_sources}")
        print(f"   - Total de se√ß√µes: {stats.total_sections}")
        print(f"   - Tempo de processamento: {stats.processing_time_seconds:.2f}s")
        
        if stats.errors:
            print(f"‚ö†Ô∏è  Erros encontrados: {len(stats.errors)}")
            for error in stats.errors[:5]:  # Mostrar apenas os primeiros 5
                print(f"   - {error}")
        
        return True
        
    except Exception as e:
        print(f"‚úó Erro durante ingest√£o: {e}")
        return False

def main():
    """Fun√ß√£o principal de recupera√ß√£o e continua√ß√£o"""
    print("üîß Iniciando recupera√ß√£o e continua√ß√£o da ingest√£o do RAG")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Passo 1: Recuperar metadados
    if not recover_metadata():
        print("‚ùå Falha na recupera√ß√£o. Abortando.")
        return
    
    # Passo 2: Continuar ingest√£o
    print("\n‚è≥ Aguarde enquanto o processo de ingest√£o continua...")
    print("üí° Este processo pode levar v√°rias horas dependendo do volume de dados")
    
    try:
        asyncio.run(continue_ingestion())
        print("\nüéâ Processo conclu√≠do com sucesso!")
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Processo interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro inesperado: {e}")

if __name__ == "__main__":
    main()